---
title: "[ML]6. 파이썬으로 Gradient, minibatch Gradient 구현해보기"
categories:
  - MachineLearning
tags:
  - ML
  - python
  - Industrial Engineering
  - Blog
excerpt: implementing Gradient Descent and minibatch gradient descent with python.
use_math: true
comments: true
---
## 0. Review
지난 포스트에서는 Linear Least Square 문제에서의 Gradient와 minibatch gradient, stochastic gradient 에 대해서 알아보았다.
이번포스트는 파이썬을 통해 Gradient , minibatch gradient, stocastic gradient 를 구현해볼 예정이다.


## 1. 데이터 받기 및 기본적인 작업

[첫 포스트](https://lookbackjh.github.io/machinelearning/Introduction-to-Machine-Learning/) 에 언급했던 수업에서 준 Homework의 Data를 이용해 파이썬으로 간단하게 Gradient descent를 이용하였습니다. 데이터 및 기본 틀은 [여기](https://bloomberg.github.io/foml/#lectures)에서 다운받을 수 있습니다. 또한, 작성한 코드를 제 [깃허브](https://github.com/lookbackjh/MLstudy) 에서 확인 할 수 있으니(보기 쉽게 실행은 Jupiter Notebook) 참고 바랍니다.(코딩이 아직 익숙하지 않아서, 좀 난잡해 보일 수 있습니다..ㅎ)

Gradient를 실행하기에 앞서서 해야할 작업들 몇가지를 먼저 살펴 봅시다.

- Feature Scaling :
  
  Feature이 여러가지가 있고, 각각의 feature이 각각 다른 단위를 가진다고 생각해 봅시다. (ex:몸무게(kg) , 키(cm)) 이렇게 다른 단위를 가지게 된다면, feature마다 다른 범위를 가지게 될 것이고, 결과값이 특정 feature에만 영향을 받게되는 상황이 자주 벌어지게 됩니다. 이를 방지하기 위해 feature의 범위를 제한시키는 것을 feature scaling이라고 합니다. 대표적으로 __Min-Max Scaling__ 이 있고, 특정 값의 위치를 최솟값과 최대값을 기준으로 어디에 있는지를 [0,1]사이에 나타낸 것입니다. 이외에도 다양한 방법이 있는데 데이터마다 적용되는 방식이 조금씩 다르다고 합니다. 더 자세한건 [여기](https://en.wikipedia.org/wiki/Feature_scaling)를 참고하면 좋을 것 같습니다. 

- Train set and Test Set:

  주어지는 데이터수는 한정되있으므로, 주어진 데이터를 특정 hyperparameter 별로 Gradient 를 시행해 weight을 구하는 train set과, 구한 weight의 성능을 평가하는 validation set , 그리고 validations set중 가장 성능이 좋았던 hyperparameter을 사용해 실제 성능을 구할 때 쓰는 data를 test data라고 합니다. 통상적으로 약 7:3 ~ 8:2 의 비율로 나눈다고 합니다. 
  원래는 전부다 하는게 맞겠지만, 다 하기는 조금 버거워서, 구현은 그냥 Train set과 test set으로 나누었습니다. 

- Code

  ````python
  import numpy as np
  import random
  from sklearn.preprocessing import MinMaxScaler
  from sklearn.model_selection import train_test_split

  def featurescale(X_train):
      ## before doing the gradient, the data must be feature scaled.., 
      ## feature이 여러가지일때, 특정 feature의 범위는 gradient 를 시행할때, 큰 영향을 줄수있다. i.e 키, 몸무게가 feature 인 경우.
      ## 따라서, feature의 크기를 어느정도 조절해주는 도구가 필요한데, 이게 feature Scaling이다.
      ## feature scaling을 함에 있어서, gradient descent의 수렴속도가 훨씬 빨라질 수 잇다. 
      ## feature scaling은 min-max scaling과 standard normal scaling이 있는데, 두가지 모두 자주 사용된다. 
      ## 두가지 방법 은 쓰임세가 살짝 다르긴한데, 대부분의 상황에서 통용될수 있는 min-max scaling을 사용하도록 하겠다.

      scalar=MinMaxScaler()
      data_num,feat_num=X_train.shape[0],X_train.shape[1]
      scalar.fit(X_train)
      X_train=scalar.transform(X_train)
      return X_train
  def splitData(X,y):
      ## 검증을 위해 데이터 X를 train 데이터와 test data로 분리시킴, 통상적으로 약 7:3 언저리 의 비율로 가름
      X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)
      return X_train,X_test,y_train,y_test
      
  ````
  직접 구현할 수도 있겠지만, sklearn에 어느정도 익숙해지고 싶어서, sklearn의 툴을 사용하였다. 사용하기 위해서는 위의 import 부분을 실행하고 [sklearn](https://scikit-learn.org/stable/)에서 적용예시 및 방법들을 보고 코드를 실행하면 된다.
  



